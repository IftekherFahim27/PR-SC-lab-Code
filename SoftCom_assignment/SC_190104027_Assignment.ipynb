{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cyeKOgwr5UC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import torch.nn as nn  # Import the nn module from torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGesiLYn9UQy",
        "outputId": "579716a2-9005-44bf-b75a-567edc07c277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "dataset_path = \"/content/drive/MyDrive/SC_190104027_Assignment1/Input\" # Total Data path\n",
        "train_output_path = \"/content/drive/MyDrive/SC_190104027_Assignment1/Output/Train\" # Train data path\n",
        "test_output_path = \"/content/drive/MyDrive/SC_190104027_Assignment1/Output/Test\" # test data path\n",
        "val_output_path = \"/content/drive/MyDrive/SC_190104027_Assignment1/Output/Validation\" # validation data path\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(train_output_path, exist_ok=True)\n",
        "os.makedirs(test_output_path, exist_ok=True)\n",
        "os.makedirs(val_output_path, exist_ok=True)\n",
        "\n",
        "# List all the class directories in the dataset path\n",
        "class_dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "\n",
        "# Shuffle the class directories to ensure randomness\n",
        "random.shuffle(class_dirs)\n",
        "\n",
        "# Split percentages\n",
        "train_percentage = 0.7\n",
        "test_percentage = 0.2\n",
        "val_percentage = 0.1\n",
        "\n",
        "\n",
        "# Loop through each class directory and copy files to appropriate splits\n",
        "for class_dir in class_dirs:\n",
        "    files = os.listdir(os.path.join(dataset_path, class_dir))\n",
        "    random.shuffle(files)\n",
        "    #print(files)\n",
        "\n",
        "    total_samples = len(files)\n",
        "    train_samples = int(train_percentage * total_samples)\n",
        "\n",
        "    test_samples = int(test_percentage * total_samples)\n",
        "\n",
        "    val_samples = total_samples - train_samples - test_samples\n",
        "\n",
        "\n",
        "    train_files = files[:train_samples]\n",
        "\n",
        "    test_files = files[train_samples:train_samples + test_samples]\n",
        "\n",
        "    val_files = files[train_samples + test_samples:train_samples + test_samples + val_samples]\n",
        "\n",
        "\n",
        "    for filename in train_files:\n",
        "        src_path = os.path.join(dataset_path, class_dir, filename)\n",
        "        dest_path = os.path.join(train_output_path, class_dir, filename)\n",
        "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
        "        shutil.copy(src_path, dest_path)\n",
        "    for filename in test_files:\n",
        "        src_path = os.path.join(dataset_path, class_dir, filename)\n",
        "        dest_path = os.path.join(test_output_path, class_dir, filename)\n",
        "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
        "        shutil.copy(src_path, dest_path)\n",
        "\n",
        "    for filename in val_files:\n",
        "        src_path = os.path.join(dataset_path, class_dir, filename)\n",
        "        dest_path = os.path.join(val_output_path, class_dir, filename)\n",
        "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
        "        shutil.copy(src_path, dest_path)\n"
      ],
      "metadata": {
        "id": "gTWDY7KRt3xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "\n",
        "# Define paths\n",
        "train_output_path = \"/content/drive/MyDrive/SC_190104027_Assignment1/Output/Train\"\n",
        "test_output_path = \"/content/drive/MyDrive/SC_190104027_Assignment1/Output/Test\"\n",
        "val_output_path = \"/content/drive/MyDrive/SC_190104027_Assignment1/Output/Validation\"\n",
        "\n",
        "# Define transformations for data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = ImageFolder(root=train_output_path, transform=transform)\n",
        "test_dataset = ImageFolder(root=test_output_path, transform=transform)\n",
        "val_dataset = ImageFolder(root=val_output_path, transform=transform)\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "batch_size =8  # Adjust as needed\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,  # Set the number of workers for data loading\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Now you can use train_loader, test_loader, and val_loader for your training, testing, and validation respectively\n"
      ],
      "metadata": {
        "id": "K44G7t1kt70F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_dataset.classes"
      ],
      "metadata": {
        "id": "qkuu9XJ1t_tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGr8mkP9uBqG",
        "outputId": "c37a31a2-fdb3-4edf-cd74-13a4b6e4b9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['COVID', 'NORMAL', 'PNEUMONIA']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_iters = 2000\n",
        "input_dim = 256*256*3\n",
        "output_dim = 3\n",
        "learning_rate = 0.001\n",
        "num_classes = 3"
      ],
      "metadata": {
        "id": "LpM6U5l6uDrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NFyGXzWuFiC",
        "outputId": "b9cbadc5-d6e1-48c8-c61e-e3a59dc13776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One Image Size\n",
        "print(train_dataset[0][0].size())\n",
        "#print(train_dataset[0][0].numpy().shape)\n",
        "# First Image Label\n",
        "#print(image_datasets['Train'][2000][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-mBTk6uuHT_",
        "outputId": "59a4b570-caff-4670-b984-6f3f9c8ffc8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.cnn_layer_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "        self.cnn_layer_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_layer_1 = nn.Linear(32 * 56 * 56, 512)  # Correct input size\n",
        "        self.linear_layer_2 = nn.Linear(512, 128)\n",
        "        self.linear_layer_3 = nn.Linear(128, num_classes)\n",
        "        self.tanh = nn.Tanh()  # Use Tanh activation function\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layer_1(x)\n",
        "        x = self.tanh(x)  # Apply Tanh activation\n",
        "        x = self.maxpool(x)\n",
        "        x = self.cnn_layer_2(x)\n",
        "        x = self.tanh(x)  # Apply Tanh activation\n",
        "        x = self.maxpool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear_layer_1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.tanh(x)  # Apply Tanh activation\n",
        "        x = self.linear_layer_2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.tanh(x)  # Apply Tanh activation\n",
        "        x = self.linear_layer_3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pv_wgyHcuJPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(num_classes=num_classes)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK_jM3KPuL19",
        "outputId": "04b35c1a-eb57-4c1b-aac6-9d39ae40a653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (cnn_layer_1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (cnn_layer_2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_layer_1): Linear(in_features=100352, out_features=512, bias=True)\n",
              "  (linear_layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (linear_layer_3): Linear(in_features=128, out_features=3, bias=True)\n",
              "  (tanh): Tanh()\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Ao_-ZRV_uO7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModel(model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total=0\n",
        "    correct = 0\n",
        "    Ttotal=0\n",
        "    Tcorrect=0\n",
        "    TT=0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Get predictions from the maximum value\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Total number of labels\n",
        "        total += labels.size(0)\n",
        "\n",
        "\n",
        "        # Total correct predictions\n",
        "        if torch.cuda.is_available():\n",
        "            correct += (predicted.cpu() == labels.cpu()).sum()\n",
        "        else:\n",
        "            correct += (predicted == labels).sum()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        # Iterate through test dataset\n",
        "\n",
        "\n",
        "    accuracy = 100 * correct.item() / total\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f' Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "gI91gX5SuPk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(5):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    trainModel(model, criterion, optimizer)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "ysBHOuRluSDX",
        "outputId": "38eda5ce-02ad-4592-b5e7-a37b55d45527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0cadf2675730>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d3c1e2a13492>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mTT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: Caught UnidentifiedImageError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 268, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 247, in pil_loader\n    img = Image.open(f)\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3283, in open\n    raise UnidentifiedImageError(msg)\nPIL.UnidentifiedImageError: cannot identify image file <_io.BufferedReader name='/content/drive/MyDrive/SC_190104027_Assignment1/Output/Train/PNEUMONIA/PNEUMONIA_1641.png'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def testModel(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    accuracy = 100 * correct.item() / total\n",
        "    average_loss = total_loss / len(loader)\n",
        "    return average_loss, accuracy\n",
        "\n",
        "# Test the model\n",
        "test_loss, test_accuracy = testModel(model, test_loader)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Validate the model\n",
        "val_loss, val_accuracy = testModel(model, val_loader)\n",
        "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJ-fvXsyxyf",
        "outputId": "eff4ad5b-5f7b-4c91-dfe6-2d6de406df67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1834, Test Accuracy: 93.3491\n",
            "Validation Loss: 0.1982, Validation Accuracy: 92.6502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "def calculate_metrics(loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "    return precision, recall, f1\n",
        "\n",
        "test_precision, test_recall, test_f1 = calculate_metrics(test_loader)\n",
        "val_precision, val_recall, val_f1 = calculate_metrics(val_loader)\n",
        "\n",
        "print(f'Test Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-Score: {test_f1:.4f}')\n",
        "print(f'Validation Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-Score: {val_f1:.4f}')\n",
        "\n",
        "# Confusion matrix\n",
        "def plot_confusion_matrix(loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(test_loader)\n",
        "plot_confusion_matrix(val_loader)\n"
      ],
      "metadata": {
        "id": "4zvLisNjt6D4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}